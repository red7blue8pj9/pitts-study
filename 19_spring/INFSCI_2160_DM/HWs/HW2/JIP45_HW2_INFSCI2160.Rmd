---
title: "JIP45_HW2_INFSCI2160"
author: "Jing Pang"
date: "1/18/2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

Goal: Explain AmountSpent in terms of the provided customer characteristics.

## Data Preparation

```{r}
direct_marketing <- read.csv("~/Dropbox/19 Spring/INFSCI_2160_DM/HWs/HW2/HOMEWORK_2_DATASET_DIRECT_MARKETING.csv")
```

## Question 1: Identifying variables

* Identify the response variable and the predictor variables.

```{r}
head(direct_marketing)
```

In this case, the response variable is AmountSpent; the predictor variables are Age, Gender, OwnHome, Married, Location, Salary, Children, History, and Catalogs

## Question 2: Exploring the dataset

### Explore the dataset and generate a statistical and graphical summary:


* There are missing values in the dataset. Describe how you deal with them. (Hint: Check the data description to see what the missingness means.)

There are 303 missing values in the History column, and around 30 percentage of History values are missing compared with total. The missing data represents a pretty huge amount fraction of the dataset. So it's probably safe just not to drop these data from my analysis. One of solutions is just to create a new category for the variable, called missing.

```{r}
# find out which column having missing values
colSums(is.na(direct_marketing))

# find summary of History column
summary(direct_marketing$History)

# create a new variable
direct_marketing$History.fix <- as.factor(ifelse(is.na(direct_marketing$History), "Missing", 
                         ifelse(direct_marketing$History=="High","High",
                                ifelse(direct_marketing$History=="Low", "Low","Medium"))))

summary(direct_marketing$History.fix)
```

* Generate a summary table for the data. For each numerical variable, list the name of it, mean, median, 1st quartile, 3rd quartile, and standard deviation.

```{r}
nums <- unlist(lapply(direct_marketing, is.numeric))
num_direct_marketing <- direct_marketing[,nums]
sapply(num_direct_marketing, function(x) c("Mean"= mean(x,na.rm=TRUE),
                             "Median" = median(x),
                             "1st Quantile" = quantile(x,0.25),
                             "3rd Quartile" = quantile(x,0.75),
                             "Std" = sd(x)
                             )
       )
```

* Plot the density distribution of the AmountSpent and Salary variables. What type of shape do they have?

```{r}
library(ggplot2)
p1 <- ggplot(direct_marketing) + geom_density(aes(x = AmountSpent),  binwidth = 100, fill = "grey", color = "black")
p2 <- ggplot(direct_marketing) + geom_density(aes(x = Salary),  binwidth = 1000, fill = "grey", color = "black")
library("gridExtra")
grid.arrange(p1, p2, ncol = 1)
```


From the graphs above, the density graph of AmountSpent feature is clearly a right-skewed shape. But the shape of Salary feature is also a kind of right-skewed shape. However, this shape spread wider than AmountSpent feature, and it has two peaks in the graph.


* Describe the relationship between all the continuous variables and the response variable in terms of correlation and a scatter plot.


In this case, because the description of dataset shows that features of Age, Gender, OwnHome, Married, Location and History are not continuous variables; we will only consider two variables Salary and Catalogs in this part.


```{r}
# correlation
cor(num_direct_marketing)

# scatter plot
library(ggplot2)
p1 <- ggplot(num_direct_marketing) + geom_point(aes(num_direct_marketing$AmountSpent, num_direct_marketing$Salary), colour = "black")
p2 <- ggplot(num_direct_marketing) + geom_point(aes(num_direct_marketing$AmountSpent, num_direct_marketing$Children), colour = "black")
p3 <- ggplot(num_direct_marketing) + geom_point(aes(num_direct_marketing$AmountSpent, num_direct_marketing$Catalogs), colour = "black")
library("gridExtra")
grid.arrange(p1, p2, p3, nrow = 2, ncol = 2)
```

* For each categorical variable, generate a conditional density plot of the response variable. (Hint: Plot the density of the response variable into multiple distributions separated by the predictorâ€™s categories on the same figure. Use different colors or line shapes to differentiate the categories.)


```{r}
sapply(direct_marketing,class)
p1<-ggplot(direct_marketing) + geom_density(aes(x = AmountSpent, color = Age))
p2<-ggplot(direct_marketing) + geom_density(aes(x = AmountSpent, color = Gender))
p3<-ggplot(direct_marketing) + geom_density(aes(x = AmountSpent, color = OwnHome))
p4<-ggplot(direct_marketing) + geom_density(aes(x = AmountSpent, color = Married))
p5<-ggplot(direct_marketing) + geom_density(aes(x = AmountSpent, color = Location))
p6<-ggplot(direct_marketing) + geom_density(aes(x = AmountSpent, color = History.fix))

# children
direct_marketing$Children.catg <- as.factor(ifelse(direct_marketing$Children==0, "Zero", 
                         ifelse(direct_marketing$Children==1,"One",
                                ifelse(direct_marketing$Children==2, "Two","Three"))))

p7<-ggplot(direct_marketing) + geom_density(aes(x = AmountSpent, color = Children.catg))

library("gridExtra")
grid.arrange(p1, p2, p3, p4, p5, p6, p7, nrow = 4, ncol = 2)
```

## Question 3: Apply regression analysis to the dataset to predict AmountSpent

### Run a multiple linear regression model. How does it perform?

##### Data transformation

We find out the features of Salary and AmountSpent are extremely huge compared to others. This will affect our model's precision. Therefore, we have to use normalization to downsize the variable into a reasonable scaling.

```{r}
# normalization dataset
# z-scale
direct_marketing$AmountSpent.norm <- scale(direct_marketing$AmountSpent, center=T,scale=T)
direct_marketing$Salary.norm <- scale(direct_marketing$Salary, center=T,scale=T)
# mean-scale
direct_marketing$Catalogs.norm <- scale(direct_marketing$Catalogs, center=T,scale=F)
```

```{r}
direct_marketing_lm <- lm(AmountSpent.norm ~ Age + Gender + OwnHome + Married + Location + Salary.norm + Children.catg + History.fix + Catalogs.norm, data = direct_marketing)
summary(direct_marketing_lm)
```


### Use ridge, lasso, and AIC for feature selection. For each model:


```{r warning=FALSE}
library(rsample)
library(glmnet)
# LASSO

## prepare train and test sets
direct_marketing.prep <- direct_marketing[,c("Age", "Gender", "OwnHome", "Married", "Location", "Children.catg", "AmountSpent.norm", "Salary.norm", "Catalogs.norm")]
direct_marketing.prep[, c("Age", "Gender", "OwnHome", "Married", "Location", "Children.catg")] <- lapply(direct_marketing.prep[, c("Age", "Gender", "OwnHome", "Married", "Location", "Children.catg")], as.factor)
direct_marketing.prep[, c("Age", "Gender", "OwnHome", "Married", "Location", "Children.catg")] <- lapply(direct_marketing.prep[, c("Age", "Gender", "OwnHome", "Married", "Location", "Children.catg")], as.numeric)

direct_marketing_train_test_split <- initial_split(direct_marketing.prep, prop = 0.8)
direct_marketing_train_tbl <- training(direct_marketing_train_test_split)
direct_marketing_test_tbl  <- testing(direct_marketing_train_test_split)
## separate variables and target

direct_marketing_vars_train<- direct_marketing_train_tbl[, c("Age", "Gender", "OwnHome", "Married", "Location", "Children.catg", "Salary.norm", "Catalogs.norm")]

direct_marketing_vars_test <- direct_marketing_test_tbl[, c("Age", "Gender", "OwnHome", "Married", "Location", "Children.catg", "Salary.norm", "Catalogs.norm")]

direct_marketing_target_train <- direct_marketing_train_tbl[, c("AmountSpent.norm")]
direct_marketing_target_test <- direct_marketing_test_tbl[, c("AmountSpent.norm")]

## cross-validation model
set.seed(1)
direct_marketing_cv_lasso = cv.glmnet(x = as.matrix(direct_marketing_vars_train), y = direct_marketing_target_train, alpha=1)
plot(direct_marketing_cv_lasso, label=TRUE)

direct_marketing.bestlasso = direct_marketing_cv_lasso$lambda.min

# RIDGE

direct_marketing_cv_ridge = cv.glmnet(x = as.matrix(direct_marketing_vars_train), y = direct_marketing_target_train, alpha=0)
plot(direct_marketing_cv_ridge, label=TRUE)
direct_marketing.bestridge <- direct_marketing_cv_ridge$lambda.min

#Test MSE of LASSO and ridge
direct_marketing.pred_lasso = predict(direct_marketing_cv_lasso, s = direct_marketing.bestlasso, newx = as.matrix(direct_marketing_vars_test))
direct_marketing.pred_lasso_mse <- mean((direct_marketing.pred_lasso-direct_marketing_test_tbl$AmountSpent)^2)

direct_marketing.pred_ridge = predict(direct_marketing_cv_ridge, s = direct_marketing.bestridge, newx = as.matrix(direct_marketing_vars_test))
direct_marketing.pred_ridge_mse <- mean((direct_marketing.pred_ridge-direct_marketing_test_tbl$AmountSpent)^2)

t_mse <- matrix(c(direct_marketing.bestlasso,direct_marketing.bestridge,direct_marketing.pred_lasso_mse,direct_marketing.pred_ridge_mse),ncol=2,byrow=TRUE)
colnames(t_mse) <- c("LASSO","RIDGE")
rownames(t_mse) <- c("lambda", "MSE")
t_mse<-as.table(t_mse)

# AIC
library(MASS)
# backward stepwise
direct_marketing_lm_backward <- stepAIC(direct_marketing_lm, direction = "backward")

# forward stepwise
direct_marketing_lm_forward <- stepAIC(direct_marketing_lm, direction = "forward")

# mix
direct_marketing_lm_both <- stepAIC(direct_marketing_lm, direction = "both")

```


* Identify which variables are statistically significant.

From the previous linear regression model, we can see that the variables Location, Salary, Children, History.fix, and Catalogs have statistically significant, while Age, Gender, OwnHome, and Married are not (all of them have p-values > 0.05)

* Evaluate the performance of your model.

```{r}
t_mse
```


The table above shows that the difference between LASSO and RIDGE is not too huge. If compared both of models, we would consider the LASSO model is performing the better result than RIDGE because of the MSE number.


```{r}
summary(direct_marketing_lm_backward)
summary(direct_marketing_lm_forward)
summary(direct_marketing_lm_both)
```


And from previous lists of summary, the forward AIC performs a better model than other two models because its F-statistic score is the best.

* Which model performed best? How did you decide this?

From the table above, we can see that the mean square error of LASSO and RIDGE feature selection model do not have too large difference. Therefore, both feature selections will work good enough.
From AIC statistics, we can see that these six variables (Gender, Location, Salary, Children, 
    History.fix and Catalogs) present a better model output.


## Question 4: Apply polynomial and locfit to the analysis

```{r}
p1<-ggplot(direct_marketing, aes(x = Gender, y = AmountSpent)) + geom_point()
p2<-ggplot(direct_marketing, aes(x = Location, y = AmountSpent)) + geom_point()
p3<-ggplot(direct_marketing, aes(x = Salary, y = AmountSpent)) + geom_point()
p4<-ggplot(direct_marketing, aes(x = Children, y = AmountSpent)) + geom_point()
p5<-ggplot(direct_marketing, aes(x = History.fix, y = AmountSpent)) + geom_point()
p6<-ggplot(direct_marketing, aes(x = Catalogs, y = AmountSpent)) + geom_point()
library("gridExtra")
grid.arrange(p1, p2, p3, p4, p5,p6, nrow = 3, ncol = 2)
```

* Apply a polynomial model to the dataset to predict AmountSpent. How does it perform? How did you choose your parameters?

```{r}
poly_direct_marketing <- data.frame(direct_marketing$Salary,direct_marketing$AmountSpent)
# Create polynomial regression models with degrees of 1, 3, 5
#Create polynomial regression with degree of 1
poly.fit <- lm(direct_marketing.AmountSpent ~ poly(direct_marketing.Salary, degree = 1), data = poly_direct_marketing)
poly_direct_marketing <- transform(poly_direct_marketing, PredictedY = predict(poly.fit))
ggplot(poly_direct_marketing , aes(x = direct_marketing.Salary, y = direct_marketing.AmountSpent)) +
  geom_point() + geom_line(data=poly_direct_marketing, aes(x = direct_marketing.Salary, y = PredictedY))

# #Create polynomial regression with degree of 3
poly.fit <- lm(direct_marketing.AmountSpent ~ poly(direct_marketing.Salary, degree = 3), data = poly_direct_marketing)
poly_direct_marketing <- transform(poly_direct_marketing, PredictedY = predict(poly.fit))
ggplot(poly_direct_marketing , aes(x = direct_marketing.Salary, y = direct_marketing.AmountSpent)) +
  geom_point() + geom_line(data=poly_direct_marketing, aes(x = direct_marketing.Salary, y = PredictedY))
# #Create polynomial regression with degree of 5
poly.fit <- lm(direct_marketing.AmountSpent ~ poly(direct_marketing.Salary, degree = 5), data = poly_direct_marketing)
poly_direct_marketing <- transform(poly_direct_marketing, PredictedY = predict(poly.fit))
ggplot(poly_direct_marketing , aes(x = direct_marketing.Salary, y = direct_marketing.AmountSpent)) +
  geom_point() + geom_line(data=poly_direct_marketing, aes(x = direct_marketing.Salary, y = PredictedY))


```

* Apply a locfit model to the dataset to predict AmountSpent. How does it perform?

The locfit model perform better and more reasonable graph than polynomial model.

```{r}
library(locfit)
# Standard regression
regular_lm <- lm(AmountSpent ~Salary, data = direct_marketing)
plot(AmountSpent~Salary, data = direct_marketing)
abline(regular_lm)
# fit a local polynomial regression model.  We will use the nearest-neighbor threshold of 0.5, or 50%
poly_fit <- locfit(AmountSpent ~ lp(Salary, nn = 0.5), data = direct_marketing)
plot(poly_fit)

```

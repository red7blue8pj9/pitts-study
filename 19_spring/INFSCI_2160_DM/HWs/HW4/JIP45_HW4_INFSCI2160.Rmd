---
title: "JIP45_HW4_INFSCI2160"
author: "Jing Pang"
date: "2/22/2019"
output: html_document
---

```{r warning=FALSE}
library(dplyr)
library(ggplot2)
library(pROC)
library(tree)
library(glmnet)
library(rsample)
library(MASS)
library(e1071)
library(randomForest)
library(class)
library(caret)
library(gridExtra)

# import dataset
telcustomer<- read.csv("~/Dropbox/19 Spring/INFSCI_2160_DM/HWs/HW4/WA_Fn-UseC_-Telco-Customer-Churn.csv")

# summary of dataset
summary(telcustomer)

# clearly there is some null values, but it is not too much, therefore, we just simply remove it from dataset
sum(is.na(telcustomer))
telcustomer_cl <- na.omit(telcustomer)

# normalization
telcustomer_cl$MonthlyCharges <- scale(telcustomer_cl$MonthlyCharges)
telcustomer_cl$TotalCharges <- scale(telcustomer_cl$TotalCharges)

# transfter target variable into numeric for later examination
telcustomer_cl$Churn_num <- ifelse(telcustomer_cl$Churn=="Yes",1,0)

summary(telcustomer_cl)

# prepare dataset 
# split dataset into train and test partitions
telcustomer_train_test_split <- initial_split(telcustomer_cl,0.8)
telcustomer_train_tbl <- training(telcustomer_train_test_split)
telcustomer_test_tbl <- testing(telcustomer_train_test_split)

```

### Question #1:
Run k-fold cross validation with a random forest model on your training data. List your most important predictors in order from most important to least. How many predictors are optimal for this dataset?

```{r}
# run random forest model on training data 
set.seed(1234)
telcustomer_train_rf_whole <- randomForest(formula = Churn ~ gender+SeniorCitizen+Partner+Dependents+tenure+PhoneService+MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+PaperlessBilling+PaymentMethod+MonthlyCharges+TotalCharges, data = telcustomer_train_tbl, importance = TRUE, ntree = 50)
telcustomer_var_importance <- data.frame(importance(telcustomer_train_rf_whole))
telcustomer_var_importance
varImpPlot(telcustomer_train_rf_whole)

# cross-validation
telcustomer_cvrf <- rfcv(telcustomer_train_tbl[,c(2:20)], telcustomer_train_tbl$Churn_num, cv.fold = 5, step = .5)
with(telcustomer_cvrf, plot(n.var, error.cv, log="x", type="o", lwd=2,
                    xlab="Number of Variables", ylab="Error Rate"))


```

From the cross-validation examination, the graph shows that the error rate is exponentially increased when the number of variables is higher than 5. Therefore, we will consider using five features for Random Forest model in this case. Reviewing the information of importance,  we believe in using these five features: tenure, TotalCharges, Contract, MonthlyCharges, InternetService.


### Question #2:
Apply different classification techniques (3 minimum) (incl. logistic regression, kNN, Naive Bayesion, decision tree, SVM, random forest) on this dataset to predict customer churn. Answer the following questions for each model (remember to use variable selection techniques where necessary!):


##### Logistic Regression

```{r}
# Logistic Regression
telcustomer_train_logit_whole <- glm(Churn ~ gender+SeniorCitizen+Partner+Dependents+tenure+PhoneService+MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+PaperlessBilling+PaymentMethod+MonthlyCharges+TotalCharges, family = binomial(link = 'logit'), data = telcustomer_train_tbl)

summary(telcustomer_train_logit_whole)
exp(telcustomer_train_logit_whole$coef[2])
exp(telcustomer_train_logit_whole$coef[3])

# Remove some features due to p-value < 0.05
telcustomer_train_logit <- glm(Churn ~ SeniorCitizen+tenure+MultipleLines+InternetService+Contract+PaperlessBilling+PaymentMethod+TotalCharges, family = binomial(link = 'logit'), data = telcustomer_train_tbl)

summary(telcustomer_train_logit)
exp(telcustomer_train_logit$coef[2])
exp(telcustomer_train_logit$coef[3])

# predict train
telcustomer_train_tbl <- telcustomer_train_tbl %>%
  mutate(P_TEST = predict(telcustomer_train_logit, newdata = telcustomer_train_tbl, type = "response"),
         P_RESP = ifelse(P_TEST >= 0.5, 1, 0))

# ROC & AUC
logit_train_roc <- roc(telcustomer_train_tbl$Churn_num, telcustomer_train_tbl$P_RESP)
logit_train_roc_plot <- plot(logit_train_roc)
logit_train_auc <- pROC::auc(logit_train_roc)
logit_train_auc
ci.auc(logit_train_auc)

# predict test
telcustomer_test_tbl <- telcustomer_test_tbl %>%
  mutate(P_TEST = predict(telcustomer_train_logit, newdata = telcustomer_test_tbl, type = "response"),
         P_RESP = ifelse(P_TEST >= 0.5, 1, 0))

# confusion matrix
logit_conf_mat <- table(telcustomer_test_tbl$Churn_num, telcustomer_test_tbl$P_RESP)

# ROC & AUC
logit_test_roc <- roc(telcustomer_test_tbl$Churn_num, telcustomer_test_tbl$P_RESP)
logit_test_roc_plot <- plot(logit_test_roc)
logit_test_auc <- pROC::auc(logit_test_roc)
logit_test_auc
ci.auc(logit_test_auc)


```

##### Decision Tree 

```{r}
# Decision Tree
telcustomer_train_dtree <- tree(Churn ~ gender+SeniorCitizen+Partner+Dependents+tenure+PhoneService+MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+PaperlessBilling+PaymentMethod+MonthlyCharges+TotalCharges, data=telcustomer_train_tbl)
summary(telcustomer_train_dtree)

# plot 
plot(telcustomer_train_dtree)
text(telcustomer_train_dtree,cex=0.75)

# tree prune
telcustomer_train_dtree_pruned <- prune.tree(telcustomer_train_dtree, best = 5)
summary(telcustomer_train_dtree_pruned)

# test
telcustomer_test_dtree_pruned <- prune.tree(telcustomer_train_dtree, best = 5, newdata = telcustomer_test_tbl)
telcustomer_test_dtree_pruned

telcustomer_test_dtree_seq = prune.tree(telcustomer_train_dtree)
plot(telcustomer_test_dtree_seq)

# also vectorize the error rates for prunings
telcustomer_test_dtree_seq$dev

# find the size of the optimal trees with respect to MSE:
telcustomer_test_dtree_optimal <- which(telcustomer_test_dtree_seq$dev == min(telcustomer_test_dtree_seq$dev))
best_num_of_leaves <- min(telcustomer_test_dtree_seq$size[telcustomer_test_dtree_optimal])
best_num_of_leaves

# final tree
telcustomer_train_dtree_final <- prune.tree(telcustomer_train_dtree, best = best_num_of_leaves)
summary(telcustomer_train_dtree_final)

# test on new prune tree
telcustomer_test_dtree_pruned_7 <- prune.tree(telcustomer_train_dtree, best = best_num_of_leaves, newdata = telcustomer_test_tbl)
summary(telcustomer_test_dtree_pruned_7)

# prediction on training data 
dtree_train_pred <- predict(telcustomer_test_dtree_pruned_7, telcustomer_train_tbl, probability = TRUE)
dtree_train_prob <- data.frame(dtree_train_pred)
dtree_train_prob$Result <- ifelse(dtree_train_prob$No > dtree_train_prob$Yes, 0, 1)

# evaluate AUC & ROC
dtree_train_roc <- roc(telcustomer_train_tbl$Churn_num, dtree_train_prob$Result)
dtree_train_roc_plot <- plot(dtree_train_roc)
dtree_train_auc <- pROC::auc(dtree_train_roc)
dtree_train_auc
ci.auc(dtree_train_auc)

# prediction on testing data 
dtree_test_pred <- predict(telcustomer_test_dtree_pruned_7, telcustomer_test_tbl, probability = TRUE)
dtree_test_prob <- data.frame(dtree_test_pred)
dtree_test_prob$Result <- ifelse(dtree_test_prob$No > dtree_test_prob$Yes, 0, 1)

# confusion matrix
dtree_conf_mat <- table(telcustomer_test_tbl$Churn_num, dtree_test_prob$Result)

# evaluate AUC & ROC
dtree_test_roc <- roc(telcustomer_test_tbl$Churn_num, dtree_test_prob$Result)
dtree_test_roc_plot <- plot(dtree_test_roc)
dtree_test_auc <- pROC::auc(dtree_test_roc)
dtree_test_auc
ci.auc(dtree_test_auc)

```

##### Support Vector Machine

```{r}
# SVM
telcustomer_train_svm <- svm(Churn ~ gender+SeniorCitizen+Partner+Dependents+tenure+PhoneService+MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+PaperlessBilling+PaymentMethod+MonthlyCharges+TotalCharges, probability = TRUE, data = telcustomer_train_tbl)

# summary 
telcustomer_train_svm
summary(telcustomer_train_svm)

svm_train_pred <- predict(telcustomer_train_svm, telcustomer_train_tbl, probability = TRUE)
svm_train_prob <- data.frame(attr(svm_train_pred, "probabilities"))
svm_train_prob$Result <- ifelse(svm_train_prob$No > svm_train_prob$Yes, 0, 1)

# evaluate AUC & ROC
svm_train_roc <- roc(telcustomer_train_tbl$Churn_num, svm_train_prob$Result)
svm_train_roc_plot <- plot(svm_train_roc)
svm_train_auc <- pROC::auc(svm_train_roc)
svm_train_auc
ci.auc(svm_train_auc)

# test predict
svm_test_pred <- predict(telcustomer_train_svm, telcustomer_test_tbl, probability = TRUE)
svm_test_prob <- data.frame(attr(svm_test_pred, "probabilities"))
svm_test_prob$Result <- ifelse(svm_test_prob$No > svm_test_prob$Yes, 0, 1)

# confusion matrix
svm_conf_mat <- table(telcustomer_test_tbl$Churn_num, svm_test_prob$Result)

# evaluate AUC & ROC 
svm_test_roc <- roc(telcustomer_test_tbl$Churn_num, svm_test_prob$Result)
svm_test_roc_plot <- plot(svm_test_roc)
svm_test_auc <- pROC::auc(svm_test_roc)
svm_test_auc
ci.auc(svm_test_auc)

```

##### Random Frost 

```{r}
# random forest in 10 features
set.seed(1234)
telcustomer_train_rf <- randomForest(formula = Churn ~ tenure+TotalCharges+Contract+MonthlyCharges+InternetService, data = telcustomer_train_tbl, importance = TRUE, ntree = 50)

# prediction on training data 
rf_train_pred <- predict(telcustomer_train_rf, telcustomer_train_tbl, probability = TRUE)
rf_train_prob <- data.frame(rf_train_pred)
rf_train_prob$Result <- ifelse(rf_train_prob$rf_train_pred =="No", 0, 1)

# evaluate AUC & ROC
rf_train_roc <- roc(telcustomer_train_tbl$Churn_num, rf_train_prob$Result)
rf_train_roc_plot <- plot(rf_train_roc)
rf_train_auc <- pROC::auc(rf_train_roc)
rf_train_auc
ci.auc(rf_train_auc)

# prediction on testing data 
rf_test_pred <-predict(telcustomer_train_rf, telcustomer_test_tbl, probability = TRUE)
rf_test_prob <- data.frame(rf_test_pred)
rf_test_prob$Result <- ifelse(rf_test_prob$rf_test_pred =="No", 0, 1)

# confusion matrix
rf_conf_mat <- table(telcustomer_test_tbl$Churn_num, rf_test_prob$Result)

# evaluate AUC & ROC
rf_test_roc <- roc(telcustomer_test_tbl$Churn_num, rf_test_prob$Result)
rf_test_roc_plot <- plot(rf_test_roc)
rf_test_auc <- pROC::auc(rf_test_roc)
rf_test_auc
ci.auc(rf_test_auc)

```



* Create an receiver operating characteristic curve for each model. What is the area under the receiver operating characteristic (AUC) for each of these models?

  * The AUC of training dataset
```{r}
# train data AUC
logit_train_auc
dtree_train_auc
svm_train_auc
rf_train_auc
```

  * The AUC of testing dataset
```{r}
# test data AUC
logit_test_auc
dtree_test_auc
svm_test_auc
rf_test_auc
```

* Which model performed best according to this metric?

From previous numbers, the AUC values of these four models present that the Logistic Regression model produce a best performance.

* How confident are you in your model? How do you quantify your confidence?

```{r}
#telcustomer_train_logit
#telcustomer_train_dtree
#telcustomer_train_svm
#telcustomer_train_rf

# train data MSE
log_train_mse <- mean((telcustomer_train_tbl$Churn_num - telcustomer_train_tbl$P_TEST)^2)
dtree_train_mse <- mean((telcustomer_train_tbl$Churn_num - dtree_train_prob$Result)^2)
svm_train_mse <- mean((telcustomer_train_tbl$Churn_num - svm_train_prob$Result)^2)
rf_train_mse <- mean((telcustomer_train_tbl$Churn_num - rf_train_prob$Result)^2)

# test data MSE
log_test_mse <- mean((telcustomer_train_tbl$Churn_num - telcustomer_test_tbl$P_TEST)^2)
dtree_test_mse <- mean((telcustomer_train_tbl$Churn_num - dtree_test_prob$Result)^2)
svm_test_mse <- mean((telcustomer_train_tbl$Churn_num - svm_test_prob$Result)^2)
rf_test_mse <- mean((telcustomer_train_tbl$Churn_num - rf_test_prob$Result)^2)


train_mse <- as.data.frame(cbind("train", log_train_mse,dtree_train_mse,svm_train_mse,rf_train_mse))
colnames(train_mse) <- c("type", "log_mse", "dtree_mse", "svm_mse", "rf_mse")
test_mse <- as.data.frame(cbind("test", log_test_mse,dtree_test_mse,svm_test_mse,rf_test_mse))
colnames(test_mse) <- c("type", "log_mse", "dtree_mse", "svm_mse", "rf_mse")

as.data.frame(rbind(train_mse,test_mse))
```

To consider which model performs better, we use MSE to evaluate the performance of models. The table above shows that although Logistic Regression is not the best model performing on training dataset, the performance on testing is best. And we also consider the values of AUC, which present us that Logistic Regression is performing best in test dataset among all these four models. Therefore, we think that, in this case, the Logistic Regression model is the best performing model in all these four models.


* Create a confusion matrix of true vs. predicted customer churn for your best model. How did you choose your threshold for classification? What is your accuracy, precision, sensitivity, specificity, F1 score, and negative predictive value for this model?

```{r}
# confusion matrix
logit_conf_mat

# plot ROC graph
plot(logit_test_roc_plot)

# find suitable threshold 
table(telcustomer_test_tbl$Churn_num)
sum(telcustomer_test_tbl$Churn_num)/length(telcustomer_test_tbl$Churn_num)
summary(telcustomer_test_tbl$P_TEST)
hist(telcustomer_test_tbl$P_TEST)

# AUC at different thresholds
telcustomer_test_logit_thresholds <- data.frame(ci.thresholds(logit_test_roc))
print(telcustomer_test_logit_thresholds)
```

In this case, we consider using Logistic Regression model as our best model. Therefore, the threshold for classification is 0.5 in this case.

And the measurements of the confusion matrix are presented in the following part.

```{r}
# Comparsion of these models
confusionMatrix(logit_conf_mat, mode = "everything", positive="1")

```



* Evaluate your confusion matrix. Name one positive aspect of the results and one negative aspect of the results.

The positive aspect of the results is that the model has better accuracy than other models. The negative aspect of the results is that the model's precision, recall, and sensitivity have a quite low value compared with others.

### Bonus question (10 pts - both parts need answered for credit):

* Create a lift table for your data. Display the table or a graph representing the table.

```{r}
# create a lift table
lift_tbl <- telcustomer_test_tbl[, c("P_TEST", "Churn_num")]
lift_tbl <- lift_tbl %>% arrange(desc(P_TEST))

# overall success probability in the evaluation dataset
baserate <- mean(telcustomer_test_tbl$Churn_num)
baserate

# prepare fields for loop function
n_test <- length(telcustomer_test_tbl$Churn_num)
ax <- 1
ay_base <- baserate
ay_pred <- lift_tbl$Churn_num[1]

# create function to make new table variables
for (i in 2:n_test) {
  ax[i] = i
  ay_base[i] = baserate*i
  ay_pred[i] = ay_pred[i-1] + lift_tbl$Churn_num[i]
}

# create new dataframe for lift plot
df=cbind(lift_tbl,ay_pred,ay_base)
df[1:20,]

plot(ax,ay_pred,xlab="number of cases",ylab="number of successes",
     main="Lift: Cum successes sorted by\n pred val/success prob")
points(ax,ay_base,type="l")

```


* Based on the totality of your analysis, how would you hypothetically suggest deploying this if you were working for this telco company?

Based on previous analysis, if the company is worry about a classification problem of Churn variable, we will suggest this telco company deploying a Logistic Regression model for predicting the variable. If the company require more detail explanations about how it decides, we will recommend deploying a Random Forest model for this task because of the precise boundary explanation produced by this model.

